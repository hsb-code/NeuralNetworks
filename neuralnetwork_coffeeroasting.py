# -*- coding: utf-8 -*-
"""Neuralnetwork_coffeeroasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ShKexy9BBx_RrHuiBHPshXef-oD2qjUC

# Installing Tensorflow
"""

# !pip install numpy

# !pip install tensorflow==2

# !pip install tensorflow==2.14.0

"""# Connect G-Drive"""

# from google.colab import drive
# drive.mount('gdrive')

"""# Import Libraries"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import keras as kr
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""# Training Dataset"""

input = [[185.32 ,12.69], [259.92 , 11.87], [231.01 , 14.41], [175.37 , 11.72],
[187.12 , 14.13] ,[225.91 , 12.1 ], [208.41 , 14.18],
[207.08 , 14.03], [280.6 ,  14.23], [202.87 , 12.25]]

X = np.array(input)

output = [[1.], [0.], [0.], [0.], [1.], [1.], [0.], [0.], [0.], [1.]]

y_train = np.array(output)

X.shape

y_train.shape

"""# Normalizing"""

# Normalization in Keras and TensorFlow refers to the process of scaling and centering input
# data to make it suitable for training machine learning models.

print(f'Temperature max and min before normalizaton, col0: {np.max(X[:, 0])} & {np.min(X[:, 0])}' )
print(f'Duration max and min before normalizaton, col1: {np.max(X[:, 1])} & {np.min(X[:, 1])}' )

normal = tf.keras.layers.Normalization(axis=-1)
normal.adapt(X) # calculate and set

# tf.keras.layers.Normalization: This is a class in TensorFlow's Keras library that represents a normalization layer#.
# It's used for normalizing input data before feeding it into a neural network.

# The line of code norm_l.adapt(X) is used to adapt (calculate and set) the normalization parameters of
# the Normalization layer (norm_l) based on the input data X. Specifically, it computes the mean and variance
# of the data in order to standardize (normalize) the input during training and later inference.

X_norm = normal(X)

print(f'Temperature max and min after normalizaton, col0: {np.max(X_norm[:, 0]):0.2f} & {np.min(X_norm[:, 0]):0.2f}' )
print(f'Duration max and min after normalizaton, col1: {np.max(X_norm[:, 1]):0.2f} & {np.min(X_norm[:, 1]):0.2f}' )

"""# Data Tiling"""

# Tiling data in NumPy can be a useful technique for increasing your dataset size by
# generating additional training examples through data augmentation. Data augmentation involves
# applying random transformations to the original data, such as rotations, translations, or flips,
# to create new, slightly modified samples. This can help improve the generalization of machine learning models.

Xt = np.tile(X_norm,(1000,1))
Yt= np.tile(y_train,(1000,1))
print(Xt.shape, Yt.shape)

"""# Tensorflow Model"""

tf.random.set_seed(1234)
model = Sequential(
    [
        tf.keras.Input(shape = (2,)), # input data should be 1D array with 2 elements
        Dense(3, activation= 'sigmoid', name = 'layer1'), # neurons, activation, name
        Dense(1, activation= 'sigmoid', name = 'layer2'),
    ]
)

# tf.random.set.seed(1234) it's essential to set the seed before any random operations occur in your code to guarantee reproducibility.

model.summary()

"""# WB Parameters"""

L1_num_params = 2 * 3 + 3   # W1 parameters  + b1 parameters
L2_num_params = 3 * 1 + 1   # W2 parameters  + b2 parameters
print("L1 params = ", L1_num_params, ", L2 params = ", L2_num_params  )

# L1_num_params = 2 features in previous layer, 3 neurons in current layer, 3 bias for current layer (each for one neuron)
# same follows for L2_num_params

w1, b1 = model.get_layer('layer1'). get_weights()
w2, b2 = model.get_layer('layer2'). get_weights()

print(f'w1{w1.shape}: {w1} & b1{b1.shape}: {b1}')

print(f'w2{w2.shape}: {w2} & b2{b2.shape}: {b2}')

"""# Training Model"""

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
)

model.fit(
    Xt,Yt,
    epochs=10,
)

# model.compile: This function is used to configure the training process of your model.
# It specifies the loss function, optimizer, and any optional metrics you want to track during training.

# loss=tf.keras.losses.BinaryCrossentropy(): This sets the loss function for your model to binary
# cross-entropy. Binary cross-entropy is commonly used as the loss function for binary classification problems,
# where the goal is to classify inputs into one of two classes (0 or 1)

# optimizer=tf.keras.optimizers.Adam(learning_rate=0.01): This sets the optimizer for your model to the Adam optimizer
# with a learning rate of 0.01. The optimizer is responsible for updating the model's weights during training
# to minimize the specified loss function.

# model.fit: This function is used to train your model on a given dataset

# epochs=10: This specifies the number of training epochs, which is the number of times the model will
# go through the entire dataset during training

"""# Updating WB"""

W1, b1 = model.get_layer("layer1").get_weights()
W2, b2 = model.get_layer("layer2").get_weights()
print("W1:\n", W1, "\nb1:", b1)
print("W2:\n", W2, "\nb2:", b2)

"""# Prediction Testing"""

X_test = np.array([
    [259,11],  # postive example
    [185,12]])   # negative example
X_testn = normal(X_test)
predictions = model.predict(X_testn)
print("predictions = \n", predictions)

yhat = np.zeros_like(predictions) # : This line initializes an array yhat with the same shape as predictions array but filled with zeros.
for i in range(len(predictions)):
    if predictions[i] >= 0.5:
        yhat[i] = 1
    else:
        yhat[i] = 0
print(f"decisions = \n{yhat}")

yhat = (predictions >= 0.5).astype(int)
print(f"decisions = \n{yhat}")